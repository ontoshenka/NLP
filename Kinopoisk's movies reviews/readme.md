# Информация об отдельных блокнотах
1. `BuildVocab.ipynb` - разбиение отзывов на токены, лемматизация токенов, построение отображения вида "токен - число" (включая, разумеется, токен `<pad>` обозначающий отсутсвия слова или наличие неизвестного слова), приведение отзывов к последовательности чисел фиксированной длины
2. `Embeddings.ipynb` - этот блокнот стоит особняком в том смысле, что тут обученые вложения токенов с помощью skip-gram, а это пока нигде больше не использовалось
3. `Modeling.ipynb` - блокнот с непосредственно решением (точнее, работой в поисках решения) задачи
4. `Without neutrals.ipynb` - блокнот с решением этой же задачи, но без нейтральных отзывов. Призван убедиться, что палки в колёса в блокноте выше нам вставляют именно нейтральные отзывы

Никаких файлов с другими вещами (вроде самих данных, созданных словарей и т.д) не привожу, поскольку они слишком много весят

# Что было сделано
Истратил кучу сил и времени на ворох непонятных ошибок, что генерировал tensorflow, перебрал ряд нейросетевых архитектур

# Грабли
Есть некоторые проблемы с пониманием того, каких размерностей данные какие слои принимают. Внимательно читайте документацию и, желательно, имейте в голове размерности входов и выходов каждого из слоёв сети. 

Указывать первую размерность как None при определении нейросети не надо - tensorflow сделает это сам. Могу ошибаться, но кажется, во всех остальных местах это делать надо.

# Чему я научился и что вынес в процессе работы
* Разобрался с базовыми принципами работы с tensorflow.
* Пощупал как изменяется время, необходимое на обучение сети изменяется в зависимости от количества рекурентных слоёв. Спойлер: 3 уже достаточно долго. Больше часа на эпоху на моей видеокарте (1660ti Max-Q design, 6 Gb). 

# Что можно сделать, чтоб улучшить результат 
1. Опробовать другие (в т.ч не рекурентные) архитектуры. Кажется, модели внимания должны отработать хорошо
2. Поварьировать веса классов 
3. Попробовать использовать предобученные вложение, тем самым отказавшись от Embedding слоя

